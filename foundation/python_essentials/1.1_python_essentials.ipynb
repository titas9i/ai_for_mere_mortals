{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8993ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID type: <class 'int'>\n",
      "Purchase history length: 4\n",
      "Profile keys: ['age', 'location', 'preferences', 'last_purchase']\n"
     ]
    }
   ],
   "source": [
    "# Essential data types for AI applications\n",
    "customer_id = 12345                    # Integer: unique identifiers\n",
    "customer_name = \"Sarah Chen\"           # String: text data for processing\n",
    "purchase_amount = 89.99               # Float: numerical data for calculations\n",
    "is_premium_member = True              # Boolean: categorical flags\n",
    "purchase_history = [45.99, 23.50, 12.99, 67.80]  # List: sequential data\n",
    "customer_profile = {                  # Dictionary: structured data\n",
    "    \"age\": 28,\n",
    "    \"location\": \"San Francisco\",\n",
    "    \"preferences\": [\"electronics\", \"books\"],\n",
    "    \"last_purchase\": \"2024-06-15\"\n",
    "}\n",
    "\n",
    "# Type checking - crucial for AI data validation\n",
    "print(f\"Customer ID type: {type(customer_id)}\")\n",
    "print(f\"Purchase history length: {len(purchase_history)}\")\n",
    "print(f\"Profile keys: {list(customer_profile.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f83b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.99, 23.5, 12.99, 67.8]\n"
     ]
    }
   ],
   "source": [
    "# Functions: Building blocks of AI data processing\n",
    "def calculate_customer_value(purchase_history, membership_multiplier=1.2):\n",
    "    \"\"\"Calculate customer lifetime value - essential for recommendation systems\"\"\"\n",
    "    if not purchase_history:\n",
    "        return 0.0\n",
    "    \n",
    "    total_spent = sum(purchase_history)\n",
    "    average_purchase = total_spent / len(purchase_history)\n",
    "    \n",
    "    # Apply membership bonus (common in AI scoring systems)\n",
    "    if membership_multiplier > 1.0:\n",
    "        total_spent *= membership_multiplier\n",
    "    \n",
    "    return round(total_spent, 2)\n",
    "\n",
    "print(purchase_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6092551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis result: {'customer_value': 164.75, 'category': 'high_value', 'feature_count': 3, 'processed_at': '2024-06-26'}\n",
      "Analyzed 1 customers\n"
     ]
    }
   ],
   "source": [
    "# Classes: Organizing complex AI logic\n",
    "class CustomerAnalyzer:\n",
    "    \"\"\"Encapsulates customer analysis logic for AI recommendations\"\"\"\n",
    "    \n",
    "    def __init__(self, premium_threshold=100.0):\n",
    "        self.premium_threshold = premium_threshold\n",
    "        self.analyzed_customers = 0\n",
    "    \n",
    "    def analyze_customer(self, customer_data):\n",
    "        \"\"\"Process customer data for AI model input\"\"\"\n",
    "        try:\n",
    "            # Extract features (common AI preprocessing step)\n",
    "            total_value = calculate_customer_value(\n",
    "                customer_data.get('purchase_history', []),\n",
    "                1.2 if customer_data.get('is_premium', False) else 1.0\n",
    "            )\n",
    "            \n",
    "            # Categorize customer (feature engineering for AI)\n",
    "            category = \"high_value\" if total_value > self.premium_threshold else \"standard\"\n",
    "            \n",
    "            self.analyzed_customers += 1\n",
    "            \n",
    "            return {\n",
    "                \"customer_value\": total_value,\n",
    "                \"category\": category,\n",
    "                \"feature_count\": len(customer_data.get('preferences', [])),\n",
    "                \"processed_at\": \"2024-06-26\"\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing customer: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Return processing statistics\"\"\"\n",
    "        return f\"Analyzed {self.analyzed_customers} customers\"\n",
    "\n",
    "# Usage example - this is how you'd use it in an AI pipeline\n",
    "analyzer = CustomerAnalyzer(premium_threshold=75.0)\n",
    "sample_customer = {\n",
    "    'purchase_history': [45.99, 23.50, 67.80],\n",
    "    'is_premium': True,\n",
    "    'preferences': ['electronics', 'books', 'music']\n",
    "}\n",
    "result = analyzer.analyze_customer(sample_customer)\n",
    "print(f\"Analysis result: {result}\")\n",
    "print(analyzer.get_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17d3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset - typical AI training data structure\n",
    "customers_data = [\n",
    "    {\"id\": 1, \"purchases\": [45.99, 23.50], \"age\": 28, \"location\": \"SF\"},\n",
    "    {\"id\": 2, \"purchases\": [12.99, 8.50, 15.75], \"age\": 34, \"location\": \"NYC\"},\n",
    "    {\"id\": 3, \"purchases\": [99.99], \"age\": 22, \"location\": \"SF\"},\n",
    "    {\"id\": 4, \"purchases\": [5.99, 7.25, 12.50, 8.75], \"age\": 45, \"location\": \"LA\"},\n",
    "    {\"id\": 5, \"purchases\": [], \"age\": 31, \"location\": \"NYC\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7fcbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer spending totals: [69.49000000000001, 37.24, 99.99, 34.49, 0]\n",
      "High-value customers: 2\n",
      "Original totals: [69.49000000000001, 37.24, 99.99, 34.49, 0]\n",
      "After discount: [62.54100000000001, 37.24, 89.991, 34.49, 0]\n",
      "Spending by location: {'SF': [69.49000000000001, 99.99], 'LA': [34.49], 'NYC': [37.24, 0]}\n"
     ]
    }
   ],
   "source": [
    "# List comprehensions: Essential for data preprocessing in AI\n",
    "\n",
    "# Extract total spending per customer (feature extraction)\n",
    "customer_totals = [sum(customer[\"purchases\"]) for customer in customers_data]\n",
    "print(f\"Customer spending totals: {customer_totals}\")\n",
    "\n",
    "# Filter high-value customers (data filtering for AI models)\n",
    "high_value_customers = [\n",
    "    customer for customer in customers_data \n",
    "    if sum(customer[\"purchases\"]) > 50.0\n",
    "]\n",
    "print(f\"High-value customers: {len(high_value_customers)}\")\n",
    "# Create feature vectors (common AI preprocessing step)\n",
    "feature_vectors = [\n",
    "    {\n",
    "        \"id\": customer[\"id\"],\n",
    "        \"total_spent\": sum(customer[\"purchases\"]),\n",
    "        \"purchase_frequency\": len(customer[\"purchases\"]),\n",
    "        \"avg_purchase\": sum(customer[\"purchases\"]) / len(customer[\"purchases\"]) if customer[\"purchases\"] else 0,\n",
    "        \"age_category\": \"young\" if customer[\"age\"] < 30 else \"mature\"\n",
    "    }\n",
    "    for customer in customers_data\n",
    "]\n",
    "# Lambda functions: Quick data transformations\n",
    "# Sort customers by value (ranking for recommendation priority)\n",
    "sorted_customers = sorted(customers_data, \n",
    "                         key=lambda x: sum(x[\"purchases\"]), \n",
    "                         reverse=True)\n",
    "# Apply discount calculation (business logic in AI systems)\n",
    "apply_discount = lambda amount: amount * 0.9 if amount > 50 else amount\n",
    "discounted_totals = list(map(apply_discount, customer_totals))\n",
    "print(f\"Original totals: {customer_totals}\")\n",
    "print(f\"After discount: {discounted_totals}\")\n",
    "# Advanced: Nested comprehensions for complex data structures\n",
    "location_summary = {\n",
    "    location: [\n",
    "        sum(customer[\"purchases\"]) \n",
    "        for customer in customers_data \n",
    "        if customer[\"location\"] == location\n",
    "    ]\n",
    "    for location in set(customer[\"location\"] for customer in customers_data)\n",
    "}\n",
    "print(f\"Spending by location: {location_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d68a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 customer records\n",
      "Analysis saved to customer_analysis.json\n",
      "\n",
      "Pipeline completed! Processed 5 customers.\n",
      "High-value customers: 0\n",
      "Average customer value: $0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# File I/O: Essential for AI data loading and model persistence\n",
    "def save_customer_analysis(analysis_results, filename=\"customer_analysis.json\"):\n",
    "    \"\"\"Save analysis results - common in AI model outputs\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(analysis_results, file, indent=2, default=str)\n",
    "        print(f\"Analysis saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving analysis: {e}\")\n",
    "\n",
    "def load_customer_data(filename=\"customers.json\"):\n",
    "    \"\"\"Load customer data - typical AI data input process\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Loaded {len(data)} customer records\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Creating sample data...\")\n",
    "        return create_sample_data()\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON format: {e}\")\n",
    "        return []\n",
    "    \n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample dataset - useful for AI experimentation\"\"\"\n",
    "    sample_data = {\n",
    "        \"customers\": customers_data,\n",
    "        \"metadata\": {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"total_customers\": len(customers_data),\n",
    "            \"data_version\": \"1.0\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save sample data for future use\n",
    "    with open(\"customers.json\", 'w') as file:\n",
    "        json.dump(sample_data, file, indent=2, default=str)\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Complete AI data processing pipeline\n",
    "def process_ai_dataset():\n",
    "    \"\"\"Complete data processing pipeline for AI applications\"\"\"\n",
    "    \n",
    "    # 1. Load data (typical AI workflow start)\n",
    "    raw_data = load_customer_data()\n",
    "    customers = raw_data.get(\"customers\", []) if isinstance(raw_data, dict) else raw_data\n",
    "    \n",
    "    # 2. Initialize analyzer\n",
    "    analyzer = CustomerAnalyzer()\n",
    "    \n",
    "    # 3. Process each customer (feature engineering)\n",
    "    analysis_results = []\n",
    "    for customer in customers:\n",
    "        result = analyzer.analyze_customer(customer)\n",
    "        if result:\n",
    "            result[\"customer_id\"] = customer[\"id\"]\n",
    "            analysis_results.append(result)\n",
    "    \n",
    "    # 4. Generate summary statistics (model evaluation metrics)\n",
    "    summary = {\n",
    "        \"total_processed\": len(analysis_results),\n",
    "        \"high_value_count\": sum(1 for r in analysis_results if r[\"category\"] == \"high_value\"),\n",
    "        \"average_value\": sum(r[\"customer_value\"] for r in analysis_results) / len(analysis_results) if analysis_results else 0,\n",
    "        \"processing_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # 5. Save results (model output persistence)\n",
    "    final_output = {\n",
    "        \"analysis_results\": analysis_results,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    save_customer_analysis(final_output)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = process_ai_dataset()\n",
    "    print(f\"\\nPipeline completed! Processed {results['summary']['total_processed']} customers.\")\n",
    "    print(f\"High-value customers: {results['summary']['high_value_count']}\")\n",
    "    print(f\"Average customer value: ${results['summary']['average_value']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac1015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 1.1 Exercise Solutions\n",
    "# AI Unleashed for Developers - Python Essentials for AI\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Exercise 1: Customer Segmentation Features\n",
    "# Objective: Learn to extract features for AI customer segmentation\n",
    "\n",
    "class CustomerAnalyzer:\n",
    "    \"\"\"Enhanced CustomerAnalyzer with behavioral features for AI segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, premium_threshold=100.0):\n",
    "        self.premium_threshold = premium_threshold\n",
    "        self.analyzed_customers = 0\n",
    "    \n",
    "    def analyze_customer(self, customer_data):\n",
    "        \"\"\"Process customer data for AI model input\"\"\"\n",
    "        try:\n",
    "            # Extract features (common AI preprocessing step)\n",
    "            total_value = self.calculate_customer_value(\n",
    "                customer_data.get('purchase_history', []),\n",
    "                1.2 if customer_data.get('is_premium', False) else 1.0\n",
    "            )\n",
    "            \n",
    "            # Categorize customer (feature engineering for AI)\n",
    "            category = \"high_value\" if total_value > self.premium_threshold else \"standard\"\n",
    "            \n",
    "            self.analyzed_customers += 1\n",
    "            \n",
    "            return {\n",
    "                \"customer_value\": total_value,\n",
    "                \"category\": category,\n",
    "                \"feature_count\": len(customer_data.get('preferences', [])),\n",
    "                \"processed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing customer: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_customer_value(self, purchase_history, membership_multiplier=1.2):\n",
    "        \"\"\"Calculate customer lifetime value - essential for recommendation systems\"\"\"\n",
    "        if not purchase_history:\n",
    "            return 0.0\n",
    "        \n",
    "        total_spent = sum(purchase_history)\n",
    "        \n",
    "        # Apply membership bonus (common in AI scoring systems)\n",
    "        if membership_multiplier > 1.0:\n",
    "            total_spent *= membership_multiplier\n",
    "        \n",
    "        return round(total_spent, 2)\n",
    "    \n",
    "    # SOLUTION: Exercise 1 - Enhanced behavioral features\n",
    "    def extract_behavioral_features(self, customer_data):\n",
    "        \"\"\"\n",
    "        Extract advanced behavioral features for AI customer segmentation\n",
    "        \n",
    "        This method demonstrates feature engineering techniques commonly used\n",
    "        in production recommendation systems and customer analytics.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Basic customer info\n",
    "        customer_id = customer_data.get('id', 'unknown')\n",
    "        purchase_history = customer_data.get('purchase_history', [])\n",
    "        preferences = customer_data.get('preferences', [])\n",
    "        last_purchase_date = customer_data.get('last_purchase_date')\n",
    "        age = customer_data.get('age', 0)\n",
    "        location = customer_data.get('location', 'unknown')\n",
    "        \n",
    "        # Feature 1: Days since last purchase (recency feature)\n",
    "        if last_purchase_date:\n",
    "            try:\n",
    "                last_date = datetime.fromisoformat(last_purchase_date)\n",
    "                days_since_last = (datetime.now() - last_date).days\n",
    "                features['days_since_last_purchase'] = days_since_last\n",
    "                \n",
    "                # Recency category (common in RFM analysis for AI)\n",
    "                if days_since_last <= 30:\n",
    "                    features['recency_category'] = 'recent'\n",
    "                elif days_since_last <= 90:\n",
    "                    features['recency_category'] = 'moderate'\n",
    "                else:\n",
    "                    features['recency_category'] = 'distant'\n",
    "            except:\n",
    "                features['days_since_last_purchase'] = 999  # Unknown/very old\n",
    "                features['recency_category'] = 'unknown'\n",
    "        else:\n",
    "            features['days_since_last_purchase'] = 999\n",
    "            features['recency_category'] = 'unknown'\n",
    "        \n",
    "        # Feature 2: Purchase diversity score (how varied are their purchases)\n",
    "        if purchase_history:\n",
    "            # Calculate coefficient of variation (std dev / mean)\n",
    "            if len(purchase_history) > 1:\n",
    "                mean_purchase = sum(purchase_history) / len(purchase_history)\n",
    "                variance = sum((x - mean_purchase) ** 2 for x in purchase_history) / len(purchase_history)\n",
    "                std_dev = variance ** 0.5\n",
    "                \n",
    "                # Diversity score: higher means more varied purchasing behavior\n",
    "                diversity_score = std_dev / mean_purchase if mean_purchase > 0 else 0\n",
    "                features['purchase_diversity_score'] = round(diversity_score, 3)\n",
    "                \n",
    "                # Categorize diversity (useful for recommendation algorithms)\n",
    "                if diversity_score < 0.3:\n",
    "                    features['purchase_pattern'] = 'consistent'\n",
    "                elif diversity_score < 0.7:\n",
    "                    features['purchase_pattern'] = 'moderate'\n",
    "                else:\n",
    "                    features['purchase_pattern'] = 'exploratory'\n",
    "            else:\n",
    "                features['purchase_diversity_score'] = 0\n",
    "                features['purchase_pattern'] = 'single_purchase'\n",
    "        else:\n",
    "            features['purchase_diversity_score'] = 0\n",
    "            features['purchase_pattern'] = 'no_purchases'\n",
    "        \n",
    "        # Feature 3: Seasonal preference analysis\n",
    "        # Note: In real applications, you'd have actual purchase dates with seasons\n",
    "        # This simulates seasonal preference based on purchase amounts and patterns\n",
    "        if purchase_history:\n",
    "            # Simulate seasonal data based on purchase patterns\n",
    "            avg_purchase = sum(purchase_history) / len(purchase_history)\n",
    "            \n",
    "            # Higher average purchases might indicate luxury/holiday buying\n",
    "            if avg_purchase > 75:\n",
    "                features['seasonal_preference'] = 'holiday_buyer'\n",
    "            elif len(purchase_history) > 5:  # Frequent purchases\n",
    "                features['seasonal_preference'] = 'year_round'\n",
    "            else:\n",
    "                features['seasonal_preference'] = 'occasional'\n",
    "        else:\n",
    "            features['seasonal_preference'] = 'inactive'\n",
    "        \n",
    "        # Feature 4: Customer lifecycle stage (based on multiple factors)\n",
    "        total_purchases = len(purchase_history)\n",
    "        total_spent = sum(purchase_history) if purchase_history else 0\n",
    "        \n",
    "        if total_purchases == 0:\n",
    "            lifecycle_stage = 'prospect'\n",
    "        elif total_purchases == 1:\n",
    "            lifecycle_stage = 'new_customer'\n",
    "        elif total_purchases < 5 and total_spent < 200:\n",
    "            lifecycle_stage = 'developing'\n",
    "        elif total_spent > 500 or total_purchases > 10:\n",
    "            lifecycle_stage = 'loyal'\n",
    "        else:\n",
    "            lifecycle_stage = 'established'\n",
    "        \n",
    "        features['lifecycle_stage'] = lifecycle_stage\n",
    "        \n",
    "        # Feature 5: Preference breadth (important for recommendation systems)\n",
    "        features['preference_breadth'] = len(preferences)\n",
    "        if len(preferences) == 0:\n",
    "            features['preference_category'] = 'undefined'\n",
    "        elif len(preferences) <= 2:\n",
    "            features['preference_category'] = 'focused'\n",
    "        elif len(preferences) <= 4:\n",
    "            features['preference_category'] = 'moderate'\n",
    "        else:\n",
    "            features['preference_category'] = 'broad'\n",
    "        \n",
    "        # Feature 6: Age-based segments (demographic feature)\n",
    "        if age < 25:\n",
    "            features['age_segment'] = 'gen_z'\n",
    "        elif age < 40:\n",
    "            features['age_segment'] = 'millennial'\n",
    "        elif age < 55:\n",
    "            features['age_segment'] = 'gen_x'\n",
    "        else:\n",
    "            features['age_segment'] = 'boomer'\n",
    "        \n",
    "        # Feature 7: Geographic category (location-based features)\n",
    "        metro_areas = ['SF', 'NYC', 'LA', 'Chicago', 'Boston', 'Seattle']\n",
    "        features['location_type'] = 'metro' if location in metro_areas else 'other'\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Return processing statistics\"\"\"\n",
    "        return f\"Analyzed {self.analyzed_customers} customers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e862aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Data Pipeline Error Handling\n",
    "# Objective: Build robust data processing for production AI systems\n",
    "\n",
    "class RobustDataPipeline:\n",
    "    \"\"\"\n",
    "    Enhanced data pipeline with comprehensive error handling for production AI systems.\n",
    "    Demonstrates best practices for handling real-world data inconsistencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_file=\"data_pipeline.log\"):\n",
    "        self.log_file = log_file\n",
    "        self.error_count = 0\n",
    "        self.processed_count = 0\n",
    "        self.warning_count = 0\n",
    "        \n",
    "    def log_message(self, level, message):\n",
    "        \"\"\"Log messages with timestamp for debugging\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        log_entry = f\"[{timestamp}] {level}: {message}\\n\"\n",
    "        \n",
    "        try:\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(log_entry)\n",
    "        except:\n",
    "            # If we can't write to log file, at least print to console\n",
    "            print(f\"LOG: {log_entry.strip()}\")\n",
    "        \n",
    "        # Also print to console for immediate feedback\n",
    "        print(f\"{level}: {message}\")\n",
    "    \n",
    "    def validate_customer_data(self, customer_data):\n",
    "        \"\"\"\n",
    "        Validate customer data structure and content.\n",
    "        Returns (is_valid, cleaned_data, warnings)\n",
    "        \"\"\"\n",
    "        warnings = []\n",
    "        cleaned_data = {}\n",
    "        \n",
    "        # Check if data is a dictionary\n",
    "        if not isinstance(customer_data, dict):\n",
    "            return False, None, [\"Data is not a dictionary\"]\n",
    "        \n",
    "        # Validate and clean required fields\n",
    "        # Customer ID\n",
    "        customer_id = customer_data.get('id')\n",
    "        if customer_id is None:\n",
    "            return False, None, [\"Missing customer ID\"]\n",
    "        \n",
    "        try:\n",
    "            cleaned_data['id'] = int(customer_id)\n",
    "        except (ValueError, TypeError):\n",
    "            return False, None, [\"Invalid customer ID format\"]\n",
    "        \n",
    "        # Purchase history\n",
    "        purchase_history = customer_data.get('purchase_history', [])\n",
    "        if not isinstance(purchase_history, list):\n",
    "            warnings.append(\"Purchase history is not a list, converting to empty list\")\n",
    "            purchase_history = []\n",
    "        \n",
    "        # Clean purchase amounts\n",
    "        cleaned_purchases = []\n",
    "        for purchase in purchase_history:\n",
    "            try:\n",
    "                amount = float(purchase)\n",
    "                if amount >= 0:  # Negative purchases don't make sense\n",
    "                    cleaned_purchases.append(amount)\n",
    "                else:\n",
    "                    warnings.append(f\"Negative purchase amount {amount} removed\")\n",
    "            except (ValueError, TypeError):\n",
    "                warnings.append(f\"Invalid purchase amount {purchase} removed\")\n",
    "        \n",
    "        cleaned_data['purchase_history'] = cleaned_purchases\n",
    "        \n",
    "        # Age validation\n",
    "        age = customer_data.get('age')\n",
    "        if age is not None:\n",
    "            try:\n",
    "                age_int = int(age)\n",
    "                if 13 <= age_int <= 120:  # Reasonable age range\n",
    "                    cleaned_data['age'] = age_int\n",
    "                else:\n",
    "                    warnings.append(f\"Age {age_int} outside reasonable range, set to None\")\n",
    "                    cleaned_data['age'] = None\n",
    "            except (ValueError, TypeError):\n",
    "                warnings.append(f\"Invalid age format {age}, set to None\")\n",
    "                cleaned_data['age'] = None\n",
    "        else:\n",
    "            cleaned_data['age'] = None\n",
    "        \n",
    "        # Location validation\n",
    "        location = customer_data.get('location', 'unknown')\n",
    "        if isinstance(location, str) and location.strip():\n",
    "            cleaned_data['location'] = location.strip()\n",
    "        else:\n",
    "            cleaned_data['location'] = 'unknown'\n",
    "            warnings.append(\"Invalid or empty location, set to 'unknown'\")\n",
    "        \n",
    "        # Preferences validation\n",
    "        preferences = customer_data.get('preferences', [])\n",
    "        if isinstance(preferences, list):\n",
    "            # Clean and validate each preference\n",
    "            cleaned_prefs = []\n",
    "            for pref in preferences:\n",
    "                if isinstance(pref, str) and pref.strip():\n",
    "                    cleaned_prefs.append(pref.strip().lower())\n",
    "            cleaned_data['preferences'] = cleaned_prefs\n",
    "        else:\n",
    "            warnings.append(\"Preferences not a list, set to empty list\")\n",
    "            cleaned_data['preferences'] = []\n",
    "        \n",
    "        # Premium membership\n",
    "        is_premium = customer_data.get('is_premium', False)\n",
    "        cleaned_data['is_premium'] = bool(is_premium)\n",
    "        \n",
    "        # Last purchase date\n",
    "        last_purchase = customer_data.get('last_purchase_date')\n",
    "        if last_purchase:\n",
    "            try:\n",
    "                # Try to parse the date\n",
    "                datetime.fromisoformat(last_purchase)\n",
    "                cleaned_data['last_purchase_date'] = last_purchase\n",
    "            except:\n",
    "                warnings.append(\"Invalid last purchase date format, set to None\")\n",
    "                cleaned_data['last_purchase_date'] = None\n",
    "        else:\n",
    "            cleaned_data['last_purchase_date'] = None\n",
    "        \n",
    "        return True, cleaned_data, warnings\n",
    "    \n",
    "    def process_customer_file(self, filename):\n",
    "        \"\"\"\n",
    "        Process a customer data file with comprehensive error handling.\n",
    "        Continues processing even when individual records fail.\n",
    "        \"\"\"\n",
    "        self.log_message(\"INFO\", f\"Starting to process file: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load the file\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.log_message(\"ERROR\", f\"File {filename} not found\")\n",
    "            return None\n",
    "        except json.JSONDecodeError as e:\n",
    "            self.log_message(\"ERROR\", f\"Invalid JSON in {filename}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.log_message(\"ERROR\", f\"Unexpected error reading {filename}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Handle different data structures\n",
    "        if isinstance(data, dict) and 'customers' in data:\n",
    "            customers = data['customers']\n",
    "        elif isinstance(data, list):\n",
    "            customers = data\n",
    "        else:\n",
    "            self.log_message(\"ERROR\", \"Data format not recognized\")\n",
    "            return None\n",
    "        \n",
    "        # Process each customer\n",
    "        analyzer = CustomerAnalyzer()\n",
    "        processed_customers = []\n",
    "        \n",
    "        for i, customer_data in enumerate(customers):\n",
    "            try:\n",
    "                # Validate and clean data\n",
    "                is_valid, cleaned_data, warnings = self.validate_customer_data(customer_data)\n",
    "                \n",
    "                if warnings:\n",
    "                    self.warning_count += len(warnings)\n",
    "                    for warning in warnings:\n",
    "                        self.log_message(\"WARNING\", f\"Customer {i}: {warning}\")\n",
    "                \n",
    "                if not is_valid:\n",
    "                    self.error_count += 1\n",
    "                    self.log_message(\"ERROR\", f\"Customer {i}: Invalid data structure\")\n",
    "                    continue\n",
    "                \n",
    "                # Process with analyzer\n",
    "                analysis_result = analyzer.analyze_customer(cleaned_data)\n",
    "                if analysis_result:\n",
    "                    # Add behavioral features\n",
    "                    behavioral_features = analyzer.extract_behavioral_features(cleaned_data)\n",
    "                    analysis_result.update(behavioral_features)\n",
    "                    analysis_result['customer_id'] = cleaned_data['id']\n",
    "                    \n",
    "                    processed_customers.append(analysis_result)\n",
    "                    self.processed_count += 1\n",
    "                else:\n",
    "                    self.error_count += 1\n",
    "                    self.log_message(\"ERROR\", f\"Customer {i}: Analysis failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                self.log_message(\"ERROR\", f\"Customer {i}: Unexpected error - {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = {\n",
    "            \"total_input_records\": len(customers),\n",
    "            \"successfully_processed\": self.processed_count,\n",
    "            \"errors\": self.error_count,\n",
    "            \"warnings\": self.warning_count,\n",
    "            \"success_rate\": (self.processed_count / len(customers)) * 100 if customers else 0\n",
    "        }\n",
    "        \n",
    "        self.log_message(\"INFO\", f\"Processing complete. Success rate: {summary['success_rate']:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"processed_customers\": processed_customers,\n",
    "            \"summary\": summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6b1d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Advanced Data Transformation\n",
    "# Objective: Master complex data manipulation for AI preprocessing\n",
    "\n",
    "class AdvancedDataTransformer:\n",
    "    \"\"\"\n",
    "    Advanced data transformation utilities for AI preprocessing.\n",
    "    Demonstrates techniques used in production ML pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_encoders = {}\n",
    "        \n",
    "    def create_one_hot_encoding(self, categories):\n",
    "        \"\"\"Create one-hot encoding mapping for categorical features\"\"\"\n",
    "        encoding_map = {}\n",
    "        for i, category in enumerate(sorted(set(categories))):\n",
    "            encoding_map[category] = i\n",
    "        return encoding_map\n",
    "    \n",
    "    def transform_customer_data_for_ml(self, customers_data):\n",
    "        \"\"\"\n",
    "        Transform customer data into ML-ready format with one-hot encoding.\n",
    "        This is the type of preprocessing you'd do before training ML models.\n",
    "        \"\"\"\n",
    "        if not customers_data:\n",
    "            return [], {}\n",
    "        \n",
    "        # Extract all unique categorical values for encoding\n",
    "        all_locations = set()\n",
    "        all_preferences = set()\n",
    "        all_age_segments = set()\n",
    "        all_lifecycle_stages = set()\n",
    "        \n",
    "        # Collect all categorical values first\n",
    "        for customer in customers_data:\n",
    "            all_locations.add(customer.get('location', 'unknown'))\n",
    "            all_preferences.update(customer.get('preferences', []))\n",
    "            all_age_segments.add(customer.get('age_segment', 'unknown'))\n",
    "            all_lifecycle_stages.add(customer.get('lifecycle_stage', 'unknown'))\n",
    "        \n",
    "        # Create encoding mappings\n",
    "        location_encoding = self.create_one_hot_encoding(all_locations)\n",
    "        preference_encoding = self.create_one_hot_encoding(all_preferences)\n",
    "        age_segment_encoding = self.create_one_hot_encoding(all_age_segments)\n",
    "        lifecycle_encoding = self.create_one_hot_encoding(all_lifecycle_stages)\n",
    "        \n",
    "        # Store encoders for future use\n",
    "        self.feature_encoders = {\n",
    "            'location': location_encoding,\n",
    "            'preferences': preference_encoding,\n",
    "            'age_segment': age_segment_encoding,\n",
    "            'lifecycle_stage': lifecycle_encoding\n",
    "        }\n",
    "        \n",
    "        # Transform each customer\n",
    "        transformed_data = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for customer in customers_data:\n",
    "            features = []\n",
    "            \n",
    "            # Numerical features (already processed)\n",
    "            numerical_features = [\n",
    "                customer.get('customer_value', 0),\n",
    "                customer.get('purchase_diversity_score', 0),\n",
    "                customer.get('days_since_last_purchase', 999),\n",
    "                customer.get('preference_breadth', 0),\n",
    "                len(customer.get('purchase_history', [])),  # frequency\n",
    "                customer.get('age', 30)  # default age\n",
    "            ]\n",
    "            features.extend(numerical_features)\n",
    "            \n",
    "            # One-hot encode location\n",
    "            location = customer.get('location', 'unknown')\n",
    "            location_vector = [0] * len(location_encoding)\n",
    "            if location in location_encoding:\n",
    "                location_vector[location_encoding[location]] = 1\n",
    "            features.extend(location_vector)\n",
    "            \n",
    "            # One-hot encode age segment\n",
    "            age_segment = customer.get('age_segment', 'unknown')\n",
    "            age_segment_vector = [0] * len(age_segment_encoding)\n",
    "            if age_segment in age_segment_encoding:\n",
    "                age_segment_vector[age_segment_encoding[age_segment]] = 1\n",
    "            features.extend(age_segment_vector)\n",
    "            \n",
    "            # One-hot encode lifecycle stage\n",
    "            lifecycle = customer.get('lifecycle_stage', 'unknown')\n",
    "            lifecycle_vector = [0] * len(lifecycle_encoding)\n",
    "            if lifecycle in lifecycle_encoding:\n",
    "                lifecycle_vector[lifecycle_encoding[lifecycle]] = 1\n",
    "            features.extend(lifecycle_vector)\n",
    "            \n",
    "            # Multi-hot encode preferences (customer can have multiple)\n",
    "            preference_vector = [0] * len(preference_encoding)\n",
    "            customer_prefs = customer.get('preferences', [])\n",
    "            for pref in customer_prefs:\n",
    "                if pref in preference_encoding:\n",
    "                    preference_vector[preference_encoding[pref]] = 1\n",
    "            features.extend(preference_vector)\n",
    "            \n",
    "            # Binary features\n",
    "            binary_features = [\n",
    "                1 if customer.get('is_premium', False) else 0,\n",
    "                1 if customer.get('category') == 'high_value' else 0,\n",
    "                1 if customer.get('location_type') == 'metro' else 0\n",
    "            ]\n",
    "            features.extend(binary_features)\n",
    "            \n",
    "            transformed_data.append(features)\n",
    "        \n",
    "        # Create feature names for interpretability\n",
    "        if not feature_names:  # Generate feature names once\n",
    "            feature_names = [\n",
    "                'customer_value', 'purchase_diversity', 'days_since_last_purchase',\n",
    "                'preference_breadth', 'purchase_frequency', 'age'\n",
    "            ]\n",
    "            \n",
    "            # Location features\n",
    "            feature_names.extend([f'location_{loc}' for loc in sorted(location_encoding.keys())])\n",
    "            \n",
    "            # Age segment features\n",
    "            feature_names.extend([f'age_segment_{seg}' for seg in sorted(age_segment_encoding.keys())])\n",
    "            \n",
    "            # Lifecycle features\n",
    "            feature_names.extend([f'lifecycle_{stage}' for stage in sorted(lifecycle_encoding.keys())])\n",
    "            \n",
    "            # Preference features\n",
    "            feature_names.extend([f'preference_{pref}' for pref in sorted(preference_encoding.keys())])\n",
    "            \n",
    "            # Binary features\n",
    "            feature_names.extend(['is_premium', 'is_high_value', 'is_metro'])\n",
    "        \n",
    "        return transformed_data, {\n",
    "            'feature_names': feature_names,\n",
    "            'encoders': self.feature_encoders,\n",
    "            'total_features': len(feature_names)\n",
    "        }\n",
    "    \n",
    "    def generate_feature_summary(self, transformed_data, metadata):\n",
    "        \"\"\"Generate summary statistics of the transformed features\"\"\"\n",
    "        if not transformed_data:\n",
    "            return {}\n",
    "        \n",
    "        import statistics\n",
    "        \n",
    "        feature_names = metadata['feature_names']\n",
    "        summary = {}\n",
    "        \n",
    "        # Calculate statistics for each feature\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            feature_values = [row[i] for row in transformed_data]\n",
    "            \n",
    "            summary[feature_name] = {\n",
    "                'mean': statistics.mean(feature_values),\n",
    "                'min': min(feature_values),\n",
    "                'max': max(feature_values),\n",
    "                'non_zero_count': sum(1 for x in feature_values if x != 0)\n",
    "            }\n",
    "            \n",
    "            # Add standard deviation for numerical features\n",
    "            if len(set(feature_values)) > 2:  # Not binary\n",
    "                try:\n",
    "                    summary[feature_name]['std'] = statistics.stdev(feature_values)\n",
    "                except:\n",
    "                    summary[feature_name]['std'] = 0\n",
    "        \n",
    "        return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db362260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exercise Solutions Demo ===\n",
      "\n",
      "1. Testing Enhanced Behavioral Features:\n",
      "Customer 1 behavioral features:\n",
      "  days_since_last_purchase: 377\n",
      "  recency_category: distant\n",
      "  purchase_diversity_score: 0.563\n",
      "  purchase_pattern: moderate\n",
      "  seasonal_preference: occasional\n",
      "  lifecycle_stage: developing\n",
      "  preference_breadth: 2\n",
      "  preference_category: focused\n",
      "  age_segment: millennial\n",
      "  location_type: metro\n",
      "\n",
      "Customer 2 behavioral features:\n",
      "  days_since_last_purchase: 403\n",
      "  recency_category: distant\n",
      "  purchase_diversity_score: 0.209\n",
      "  purchase_pattern: consistent\n",
      "  seasonal_preference: occasional\n",
      "  lifecycle_stage: developing\n",
      "  preference_breadth: 3\n",
      "  preference_category: moderate\n",
      "  age_segment: millennial\n",
      "  location_type: metro\n",
      "\n",
      "Customer 3 behavioral features:\n",
      "  days_since_last_purchase: 372\n",
      "  recency_category: distant\n",
      "  purchase_diversity_score: 0.2\n",
      "  purchase_pattern: consistent\n",
      "  seasonal_preference: holiday_buyer\n",
      "  lifecycle_stage: established\n",
      "  preference_breadth: 1\n",
      "  preference_category: focused\n",
      "  age_segment: gen_z\n",
      "  location_type: metro\n",
      "\n",
      "2. Testing Robust Error Handling:\n",
      "INFO: Starting to process file: test_customers.json\n",
      "WARNING: Customer 3: Invalid purchase amount invalid removed\n",
      "WARNING: Customer 3: Negative purchase amount -10.0 removed\n",
      "WARNING: Customer 3: Invalid age format thirty, set to None\n",
      "WARNING: Customer 3: Invalid or empty location, set to 'unknown'\n",
      "WARNING: Customer 3: Preferences not a list, set to empty list\n",
      "ERROR: Customer 3: Unexpected error - '<' not supported between instances of 'NoneType' and 'int'\n",
      "WARNING: Customer 4: Missing customer ID\n",
      "ERROR: Customer 4: Invalid data structure\n",
      "INFO: Processing complete. Success rate: 60.0%\n",
      "Processing Summary:\n",
      "  total_input_records: 5\n",
      "  successfully_processed: 3\n",
      "  errors: 2\n",
      "  warnings: 6\n",
      "  success_rate: 60.0\n",
      "\n",
      "Successfully processed 3 customers\n",
      "\n",
      "3. Testing Advanced Data Transformation:\n",
      "Transformed 3 customers into ML-ready format\n",
      "Total features per customer: 14\n",
      "Feature categories: 4 categorical encodings\n",
      "\n",
      "Sample transformed customer (first 10 features):\n",
      "Features: [180.34, 0.563, 377, 2, 0, 30, 1, 0, 1, 1]\n",
      "Feature names: ['customer_value', 'purchase_diversity', 'days_since_last_purchase', 'preference_breadth', 'purchase_frequency', 'age', 'location_unknown', 'age_segment_gen_z', 'age_segment_millennial', 'lifecycle_developing']\n",
      "\n",
      "Feature Summary (first 5 features):\n",
      "  customer_value: mean=167.27, min=21.49, max=299.99\n",
      "  purchase_diversity: mean=0.32, min=0.2, max=0.563\n",
      "  days_since_last_purchase: mean=384.00, min=372, max=403\n",
      "  preference_breadth: mean=2.00, min=1, max=3\n",
      "  purchase_frequency: mean=0.00, min=0, max=0\n",
      "\n",
      "=== All Exercises Completed Successfully! ===\n",
      "Check 'test_pipeline.log' for detailed processing logs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample test data and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data with various edge cases for testing\n",
    "    sample_customers = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"purchase_history\": [45.99, 23.50, 67.80, 12.99],\n",
    "            \"age\": 28,\n",
    "            \"location\": \"SF\",\n",
    "            \"preferences\": [\"electronics\", \"books\"],\n",
    "            \"is_premium\": True,\n",
    "            \"last_purchase_date\": \"2024-06-15\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"purchase_history\": [12.99, 8.50],\n",
    "            \"age\": 34,\n",
    "            \"location\": \"NYC\",\n",
    "            \"preferences\": [\"music\", \"movies\", \"books\"],\n",
    "            \"is_premium\": False,\n",
    "            \"last_purchase_date\": \"2024-05-20\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"purchase_history\": [99.99, 150.00],\n",
    "            \"age\": 22,\n",
    "            \"location\": \"LA\",\n",
    "            \"preferences\": [\"electronics\"],\n",
    "            \"is_premium\": True,\n",
    "            \"last_purchase_date\": \"2024-06-20\"\n",
    "        },\n",
    "        # Edge cases for testing error handling\n",
    "        {\n",
    "            \"id\": \"4\",  # String ID (should be converted)\n",
    "            \"purchase_history\": [\"invalid\", 25.50, -10.00],  # Mixed invalid data\n",
    "            \"age\": \"thirty\",  # Invalid age\n",
    "            \"location\": \"\",  # Empty location\n",
    "            \"preferences\": \"not a list\",  # Invalid preferences\n",
    "            \"is_premium\": \"yes\"  # String boolean\n",
    "        },\n",
    "        {\n",
    "            # Missing ID - should be rejected\n",
    "            \"purchase_history\": [50.00],\n",
    "            \"age\": 25\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Exercise Solutions Demo ===\\n\")\n",
    "    \n",
    "    # Test Exercise 1: Enhanced behavioral features\n",
    "    print(\"1. Testing Enhanced Behavioral Features:\")\n",
    "    analyzer = CustomerAnalyzer()\n",
    "    \n",
    "    for customer in sample_customers[:3]:  # Use valid customers only\n",
    "        features = analyzer.extract_behavioral_features(customer)\n",
    "        print(f\"Customer {customer['id']} behavioral features:\")\n",
    "        for key, value in features.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "    \n",
    "    # Test Exercise 2: Robust error handling\n",
    "    print(\"2. Testing Robust Error Handling:\")\n",
    "    \n",
    "    # Save sample data to file for testing\n",
    "    with open('test_customers.json', 'w') as f:\n",
    "        json.dump({\"customers\": sample_customers}, f, indent=2)\n",
    "    \n",
    "    pipeline = RobustDataPipeline(\"test_pipeline.log\")\n",
    "    result = pipeline.process_customer_file('test_customers.json')\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Processing Summary:\")\n",
    "        for key, value in result['summary'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\nSuccessfully processed {len(result['processed_customers'])} customers\")\n",
    "    \n",
    "    # Test Exercise 3: Advanced data transformation\n",
    "    print(\"\\n3. Testing Advanced Data Transformation:\")\n",
    "    \n",
    "    if result and result['processed_customers']:\n",
    "        transformer = AdvancedDataTransformer()\n",
    "        transformed_data, metadata = transformer.transform_customer_data_for_ml(\n",
    "            result['processed_customers']\n",
    "        )\n",
    "        \n",
    "        print(f\"Transformed {len(transformed_data)} customers into ML-ready format\")\n",
    "        print(f\"Total features per customer: {metadata['total_features']}\")\n",
    "        print(f\"Feature categories: {len(metadata['encoders'])} categorical encodings\")\n",
    "        \n",
    "        # Show sample transformed customer\n",
    "        if transformed_data:\n",
    "            print(f\"\\nSample transformed customer (first 10 features):\")\n",
    "            print(f\"Features: {transformed_data[0][:10]}\")\n",
    "            print(f\"Feature names: {metadata['feature_names'][:10]}\")\n",
    "            \n",
    "        # Generate and display feature summary\n",
    "        summary = transformer.generate_feature_summary(transformed_data, metadata)\n",
    "        print(f\"\\nFeature Summary (first 5 features):\")\n",
    "        for i, (feature_name, stats) in enumerate(list(summary.items())[:5]):\n",
    "            print(f\"  {feature_name}: mean={stats['mean']:.2f}, min={stats['min']}, max={stats['max']}\")\n",
    "    \n",
    "    print(\"\\n=== All Exercises Completed Successfully! ===\")\n",
    "    print(\"Check 'test_pipeline.log' for detailed processing logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
